# AUTOGENERATED! DO NOT EDIT! File to edit: 01_callback.core.ipynb (unless otherwise specified).

__all__ = ['Events', 'Callback']

# Cell
from ..foundations import DocumentedEnum, noop

from fastcore.basics import mk_class, class2attr, GetAttr, Stateful, store_attr, detuplify
from fastcore.foundation import L
from fastcore.meta import funcs_kwargs

# Cell
class Events(DocumentedEnum):
    "All possible Callback events"
    after_create = "Called after the `Learner` is created"
    before_fit = "Called before starting training or inference, ideal for initial setup"
    before_epoch = "Called at the beginning of each epoch, useful for any behavior you need to reset at each epoch"
    before_train = "Called at the beginning of each training part of an epoch"
    before_batch = "Called at the beginning of each batch, just after drawing said batch. It can be used to do any setup necessary for the batch (like hyper-parameter scheduling) or to change the input/target before it goes in the model (change of the input with techniques like mixup for instance)."
    after_pred = "Called after computing the output of the model on the batch. It can be used to change that output before it's fed to the loss."
    after_loss = "Called after the loss has been computed, but before the backward pass. It can be used to add any penalty to the loss (AR or TAR in RNN training for instance)."
    before_backward = "Called after the loss has been computed, but only in training mode (i.e. when the backward pass will be used)"
    before_step = "Called after the backward pass, but before the update of the parameters. It can be used to do any change to the gradients before said update (gradient clipping for instance)."
    after_step = "Called after the step and before the gradients are zeroed."
    after_batch = "Called at the end of a batch, for any clean-up before the next one."
    after_cancel_train = "Reached immediately after a `CancelTrainException` before proceeding to `after_epoch`"
    after_train = "Called at the end of the training phase of an epoch."
    before_validate = "Called at the beginning of the validation phase of an epoch, useful for any setup needed specifically for validation."
    after_cancel_validate = "Reached immediately after a `CancelValidException` before proceeding to `after_epoch`"
    after_validate = "Called at the end of the validation part of an epoch."
    after_cancel_epoch = "Reached immediately after a `CancelEpochException` before proceeding to `after_epoch`"
    after_epoch = "Called at the end of an epoch, for any clean-up before the next one."
    after_cancel_fit = "Reached immediately after a `CancelFitException` before proceeding to `after_fit`"
    after_fit = "Called at the end of training, for final clean-up."

# Cell
_inner_loop = "before_batch after_pred after_loss before_backward before_step after_step after_cancel_batch after_batch".split()

# Cell
@funcs_kwargs(as_method=True)
class Callback(Stateful,GetAttr):
    """
    Basic class handling tweaks of the training loop by changing a `Learner` in various events.

    To use, implement any supported event in `Events` that should be called.
    An `order` can be passed to dictate its call priority
    """
    order,_default,learn,run,run_train,run_valid = 0,'learn',None,True,True,True
    _methods = [e.name for e in Events]

    def __init__(self, **kwargs): assert not kwargs, f'Passed unknown events: {kwargs}'
    def __repr__(self): return type(self).__name__

    def __call__(self, event_name):
        "Call `self.{event_name}` if it's defined"
        _run = (event_name not in _inner_loop or (self.run_train and getattr(self, 'training', True)) or
               (self.run_valid and not getattr(self, 'training', False)))
        res = None
        if self.run and _run:
            try:
                res = getattr(self, event_name, noop)()
            except Exception as e:
                e.args = [f'Exception occured when calling event `{event_name}` in `{self.name}`:\n\t{e.args[0]}']
                raise e
        if event_name=='after_fit': self.run=True #Reset self.run to True at each end of fit
        return res

    def __setattr__(self, name, value):
        if hasattr(self.learn,name):
            warn(f"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this")
        super().__setattr__(name, value)

    @property
    def name(self):
        "Name of the `Callback`, camel-cased and with '*Callback*' removed"
        return class2attr(self, 'Callback').title()